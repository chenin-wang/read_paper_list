## [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) 发表时间: 2021-03-25

本文提出了一种新的视觉Transformer，称为Swin Transformer，它可以作为计算机视觉的通用骨干网络。将Transformer从语言适应到视觉的挑战源于这两个领域之间的差异，例如视觉实体的规模变化很大，以及图像中像素的分辨率远远高于文本中的单词。为了解决这些差异，我们提出了一种层次化的Transformer，其表示是使用**移位窗口**计算的。移位窗口方案通过将自注意力计算限制在非重叠的局部窗口内来提高效率，同时也允许跨窗口连接。这种层次结构具有在各种尺度上建模的灵活性，并且相对于图像大小具有线性计算复杂度。Swin Transformer的这些特性使其能够兼容广泛的视觉任务，包括图像分类（ImageNet-1K上top-1准确率为87.3%）和密集预测任务，如目标检测（COCO test-dev上box AP为58.7，mask AP为51.1）和语义分割（ADE20K val上mIoU为53.5）。它的性能大大超过了之前的最佳水平，在COCO数据集上box AP和mask AP分别提升了+2.7和+2.6，在ADE20K数据集上mIoU提升了+3.2，证明了基于Transformer的模型作为视觉骨干网络的潜力。分层设计和移位窗口方法也被证明有利于所有MLP架构。代码和模型已公开发布在~\url{https://github.com/microsoft/Swin-Transformer}。 


---

## [MAC-VO: Metrics-aware Covariance for Learning-based Stereo Visual Odometry](https://arxiv.org/abs/2409.09479v1) 发表时间: 2024-09-14

我们提出了MAC-VO，一种新颖的基于学习的立体视觉里程计，它利用学习的度量感知匹配不确定性来实现双重目的：选择关键点和在位姿图优化中加权残差。与传统的几何方法优先考虑富含纹理的特征（如边缘）不同，我们的关键点选择器采用学习的不确定性，根据全局不一致性过滤掉低质量的特征。与为协方差矩阵建模尺度无关的对角线权重矩阵的基于学习的算法相比，我们设计了一种度量感知协方差模型来捕获关键点配准过程中的空间误差以及不同轴之间的相关性。将此协方差模型集成到位姿图优化中，增强了位姿估计的鲁棒性和可靠性，特别是在具有变化的光照、特征密度和运动模式的挑战性环境中。在公共基准数据集上，MAC-VO优于现有的VO算法，甚至在具有挑战性的环境中优于某些SLAM算法。协方差图还提供了有关估计位姿的可靠性的宝贵信息，这有利于自主系统的决策。 


---

